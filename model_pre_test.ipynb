{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import success\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_rows\", 200)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier, Perceptron\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "print(\"import success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data => rows: 74250, cols: 114\n",
      "labels => rows: 59400, cols: 1\n"
     ]
    }
   ],
   "source": [
    "# import pre-processed datasets\n",
    "\n",
    "# train data\n",
    "data = pd.read_csv(r\"source/for_train.csv\", index_col=\"id\")\n",
    "\n",
    "# target\n",
    "labels = pd.read_csv(r\"source/train_labels.csv\", index_col=\"id\")\n",
    "\n",
    "# check whether rows are equal\n",
    "print(\"data => rows: %s, cols: %s\" % (data.shape[0], data.shape[1]))\n",
    "print(\"labels => rows: %s, cols: %s\" % (labels.shape[0], labels.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = labels.values.ravel()\n",
    "\n",
    "train = data[data.type.eq(\"train\")].drop(\"type\", axis=1)\n",
    "test = data[data.type.eq(\"test\")].drop(\"type\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train,\n",
    "    target,\n",
    "    test_size = 0.2,\n",
    "    shuffle = True,\n",
    "    stratify = target,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "size = len(y_train)\n",
    "X_train_pruned = X_train.head(size).copy()\n",
    "y_train_pruned = copy.deepcopy(y_train[:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select models for pre-mature testing\n",
    "\n",
    "# Reference: https://towardsdatascience.com/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362\n",
    "\n",
    "models = [\n",
    "    BernoulliNB(),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    ExtraTreeClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    LinearSVC(multi_class = 'crammer_singer'),\n",
    "    LogisticRegression(multi_class=\"multinomial\"),\n",
    "    # LogisticRegressionCV(multi_class=\"multinomial\"),\n",
    "    RandomForestClassifier(criterion=\"entropy\"),\n",
    "    AdaBoostClassifier(),\n",
    "    XGBClassifier(),\n",
    "    XGBRFClassifier(),\n",
    "    HistGradientBoostingClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    CatBoostClassifier(verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed model: <catboost.core.CatBoostClassifier object at 0x000002DD3EE240C8>\n",
      "Completed model: RandomForestClassifier(criterion='entropy', n_jobs=4)\n",
      "Completed model: LGBMClassifier(n_jobs=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OpenSoftware\\miniconda3\\envs\\pump_it_up\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:05:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Completed model: XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "--------------------------------------------\n",
      "RandomForestClassifier(criterion='entropy', n_jobs=4):\t0.795930235349091\n",
      "<catboost.core.CatBoostClassifier object at 0x000002DD3EE240C8>:\t0.7815671070480033\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None):\t0.7776228703571458\n",
      "LGBMClassifier(n_jobs=4):\t0.7705512503062181\n"
     ]
    }
   ],
   "source": [
    "# pre-mature testing to select a good model\n",
    "def fit_predict(model):\n",
    "    model.fit(X_train_pruned, y_train_pruned)\n",
    "    predicted_vals = model.predict(X_test)\n",
    "    score = f1_score(y_test, predicted_vals, average=\"weighted\")\n",
    "\n",
    "    return model, score\n",
    "\n",
    "result = []\n",
    "for model in models:\n",
    "    result.append(fit_predict(model))\n",
    "    print(\"Completed model: %s\" % model)\n",
    "\n",
    "# summary\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "result.sort(key = lambda i: i[-1], reverse=True)\n",
    "\n",
    "for model, score in result:\n",
    "    print(\"%s:\\t%s\" %(model, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that only first 3000 rows are used to fit and test different classifiers\n",
    "\n",
    "Some of them are resource hungry, so I had to limit row count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Results are as follows:\n",
    "\n",
    "CatBoostClassifier()                            0.7235016674367357\n",
    "RandomForestClassifier():\t                    0.7220672374568503\n",
    "LGBMClassifier():\t                            0.720531724640514\n",
    "XGBClassifier():\t                            0.7194778397818585\n",
    "HistGradientBoostingClassifier():\t            0.7189712814704061\n",
    "ExtraTreesClassifier():\t                        0.7127838117079437\n",
    "LogisticRegression(multi_class='multinomial'):\t0.6924761104786776\n",
    "LinearSVC(multi_class='crammer_singer'):\t    0.6903410937095616\n",
    "XGBRFClassifier():\t                            0.6849082996886071\n",
    "AdaBoostClassifier():\t                        0.6768951452216663\n",
    "DecisionTreeClassifier():\t                    0.6695058355336658\n",
    "ExtraTreeClassifier():\t                        0.6680078146995786\n",
    "KNeighborsClassifier():\t                        0.6672741948775417\n",
    "BernoulliNB():\t                                0.6658124805827229\n",
    "GaussianNB():\t                                0.141613926068683\n",
    "\n",
    "From this observation, it can be concluded that CatBoostClassifier()\n",
    "performs best with basic settings.\n",
    "\n",
    "But following are can be given a re-match with higher number (gave full dataset) of\n",
    "rows and results are here:\n",
    "RandomForestClassifier(criterion='entropy', n_jobs=4):\t0.795930235349091\n",
    "CatBoostClassifier():\t                                0.7815671070480033\n",
    "XGBClassifier():\t                                    0.7776228703571458\n",
    "LGBMClassifier(n_jobs=4):\t                            0.7705512503062181\n",
    "\n",
    "\n",
    "following classifiers didn't work because of not enough\n",
    "system resources / configuration issues\n",
    "\n",
    "1. LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# These are for one-vs-one classification\n",
    "models = [\n",
    "    # NuSVC(),\n",
    "    SVC(),\n",
    "    GaussianProcessClassifier(multi_class=\"one_vs_one\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pre-mature testing to select a good model\n",
    "def fit_predict(model):\n",
    "    ovo = OneVsOneClassifier(estimator=model, n_jobs=1)\n",
    "    ovo.fit(X_train_pruned, y_train_pruned)\n",
    "    predicted_vals = ovo.predict(X_test)\n",
    "    score = f1_score(y_test, predicted_vals, average=\"weighted\")\n",
    "\n",
    "    return model, score\n",
    "\n",
    "# result = []\n",
    "# for model in models:\n",
    "#     result.append(fit_predict(model))\n",
    "#     print(\"Completed model: %s\" % model)\n",
    "#\n",
    "# # summary\n",
    "# print(\"--------------------------------------------\")\n",
    "#\n",
    "# result.sort(key = lambda i: i[-1], reverse=True)\n",
    "#\n",
    "# for model, score in result:\n",
    "#     print(\"%s:\\t%s\" %(model, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Results are as follows:\n",
    "\n",
    "SVC():\t                                                0.7086297175832617\n",
    "GaussianProcessClassifier(multi_class='one_vs_one'):\t0.6934369848273506\n",
    "\n",
    "From this observation, it can be concluded that SVC()\n",
    "performs best with basic settings\n",
    "\n",
    "following classifiers didn't work because of not enough\n",
    "system resources / configuration issues\n",
    "\n",
    "1. NuSVC()\n",
    "2. GaussianProcessClassifier(multi_class=\"one_vs_one\") -> could not handle large data. My system is not capable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# These are for one-vs-rest classification\n",
    "models = [\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianProcessClassifier(multi_class=\"one_vs_rest\"),\n",
    "    LinearSVC(multi_class=\"ovr\"),\n",
    "    LogisticRegression(multi_class=\"ovr\"),\n",
    "    # LogisticRegressionCV(multi_class=\"ovr\"),\n",
    "    SGDClassifier(),\n",
    "    Perceptron()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pre-mature testing to select a good model\n",
    "def fit_predict(model):\n",
    "    ovo = OneVsRestClassifier(estimator=model, n_jobs=1)\n",
    "    ovo.fit(X_train_pruned, y_train_pruned)\n",
    "    predicted_vals = ovo.predict(X_test)\n",
    "    score = f1_score(y_test, predicted_vals, average=\"weighted\")\n",
    "\n",
    "    return model, score\n",
    "\n",
    "# result = []\n",
    "# for model in models:\n",
    "#     result.append(fit_predict(model))\n",
    "#     print(\"Completed model: %s\" % model)\n",
    "#\n",
    "# # summary\n",
    "# print(\"--------------------------------------------\")\n",
    "#\n",
    "# result.sort(key = lambda i: i[-1], reverse=True)\n",
    "#\n",
    "# for model, score in result:\n",
    "#     print(\"%s:\\t%s\" %(model, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are as follows:\n",
    "\n",
    "GradientBoostingClassifier():\t        0.7002652846652517\n",
    "GaussianProcessClassifier():\t        0.6966250634142289\n",
    "LogisticRegression(multi_class='ovr'):\t0.6908173731656669\n",
    "LinearSVC():\t                        0.6893327474872507\n",
    "SGDClassifier():\t                    0.6877661844945466\n",
    "Perceptron():\t                        0.6620726289681607\n",
    "\n",
    "From this observation, it can be concluded that GradientBoostingClassifier()\n",
    "performs best with basic settings\n",
    "\n",
    "following classifiers didn't work because of not enough\n",
    "system resources / configuration issues\n",
    "\n",
    "1. LogisticRegressionCV(multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Overall, RandomForestClassifier() is chosen as best classifier in given conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are as follows:\n",
    "\n",
    "SVC() 0.7588050231479048\n",
    "\n",
    "From this observation, it can be concluded that ExtraTreesClassifier()\n",
    "performs best with basic settings\n",
    "\n",
    "following classifiers didn't work because of not enough\n",
    "system resources / configuration issues\n",
    "\n",
    "1. NuSVC()\n",
    "2. GaussianProcessClassifier(multi_class=\"one_vs_one\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}